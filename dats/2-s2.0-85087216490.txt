approaches to discourse analysis in Alzheimer's disease
Natural Language Processing (NLP) is an ever-growing field of computational science that aims to model natural human language.
Combined with advances in machine learning, which learns patterns in data, it offers practical capabilities including automated language analysis.
These approaches have garnered interest from clinical researchers seeking to understand the breakdown of language due to pathological changes in the brain, offering fast, replicable and objective methods.
The study of Alzheimer's disease (AD), and preclinical Mild Cognitive Impairment (MCI), suggests that changes in discourse (connected speech or writing) may be key to early detection of disease.
There is currently no disease-modifying treatment for AD, the leading cause of dementia in people over the age of 65, but detection of those at risk of developing the disease could help with the identification and testing of medications which can take effect before the underlying pathology has irreversibly spread.
We outline important components of natural language, as well as NLP tools and approaches with which they can be extracted, analysed and used for disease identification and risk prediction.
We review literature using these tools to model discourse across the spectrum of AD, including the contribution of machine learning approaches and Automatic Speech Recognition (ASR).
We conclude that NLP and machine learning techniques are starting to greatly enhance research in the field, with measurable and quantifiable language components showing promise for early detection of disease, but there remain research and practical challenges for clinical implementation of these approaches.
Challenges discussed include the availability of large and diverse datasets, ethics of data collection and sharing, diagnostic specificity and clinical acceptability.

Introduction
Natural Language Processing (NLP) and machine learning have changed the way humans and computers interact, making language-processing applications a familiar part of everyday life.
Alexa, Siri, and Google Translate all depend on machine learning and NLP algorithms.
The growth of NLP has been attributed to recent advances in machine learning algorithms, made possible by greater distributed computing power, large amounts of data available in digital form, and a deeper understanding of the structure of human languages
It is clear, however, that adoption of the technology in the clinical domain is undoubtedly beginning to transform our ability to assess neurodegenerative diseases such as Alzheimer's disease (AD).
Evidence suggests that the build-up of pathology in AD begins decades before symptoms emerge
Detection of mild cognitive impairment (MCI) is particularly pertinent, as MCI is associated with a ~15% annual risk of dementia compared to 1-2% in unimpaired elderly
MCI therefore represents an at-risk state for future AD.
A reliable AD biomarker (a quantifiable change that correlates with pathological load) would undoubtedly lead to early recognition of disease, but available biomarkers (cerebrospinal fluid assays and amyloid ligand imaging) are invasive, timeconsuming and expensive, and therefore not currently candidates for routine or large scale testing
Brief cognitive screening tests such as the Mini Mental State Examination (MMSE)
They include minimal assessment of language ability, despite it long being recognised as a feature of AD: Faber-Langendoen et al.
In the MCI phase, the inclusion of language-based measures in assessment improves accuracy in predicting progression to AD
Demonstration of the linguistic changes of AD could therefore be a sensitive marker of early detection of cognitive decline
There is growing interest in naturally produced language in the form of samples of writing or speech, which are very easily collected and may be more representative of problems encountered in everyday life for individuals living with AD (López-
As manual scoring is slow and reliant on subjective judgement, a key requirement is a means of analysing and interpreting such data rapidly, reliably and at scale
NLP and machine learning approaches meet these aims, and could lead to 'flag raising' systems which identify those at risk of disease, such as those at the MCI stage.
With the additional potential for remote monitoring, the nature of ongoing assessment could evolve: regular monitoring could be accomplished without the need for hospital visits and without the practice effects that can make cognitive assessment difficult to interpret.
Clinical trial methodologies, which currently depend on two or more years of follow-up, could also be revolutionised, leading to shorter, more efficient testing of new dementia treatments.
Although not the focus of this review, we should mention in passing the growing interest in the application of NLP to the large-scale identification and extraction of relevant clinical data from Electronic Health Records (EHRs).
In the dementia field this has been applied to identify subsets of patients already diagnosed with dementia, such as those suitable for a clinical trial
Recent collaborative projects are enabling access to thousands of medical records, with overarching goals that include harnessing this complex data, along with other sources, to aid early diagnosis or increase its accuracy, such as identifying features of misdiagnosis.
Dementias Platform UK (DPUK;
enables access to an online portal of rich cohort data, while project iASiS (
has a particular focus on NLP techniques, mining EHRs and other records for information that will lead to better decision making at individual and policy levels
This review outlines the contributions of machine learning and NLP to the problem of dementia detection, and is structured according to discourse properties of potential importance.
These are considered under the broad headings of individual words (vocabulary), and overall structure (connected language).
After defining each feature we review methods and tools for their extraction (summarised in Table
This is followed by an overview of newer machine learning methods and Automatic Speech Recognition (ASR).
To orient the reader to the clinical context, Table

Vocabulary
One of the simplest approaches to analysing language is to examine vocabulary, which provides information about the specific kinds of words people use and how those words relate to expected norms of the language being studied.
A traditional starting point is the 'bag-of-words' assumption, under which the words in a discourse sample are considered without reference to the order in which they were produced, leaving their inherent lexical or grammatical properties as variables of interest

Lexical properties
The most commonly occurring words in any corpus are grammatical function words, or 'closed class', which indicate how a sentence is structured irrespective of its topic, while meaning is provided by content or 'open class' words.
Content bearing nouns and verbs have a number of associated lexical properties, the analysis of which can provide information about the complexity of the vocabulary.
Lexical properties include measures of the frequency with which a word appears in discourse, its familiarity to speakers of the language
For example, the word 'ELEPHANT' tends to be acquired early in life and can be easily pictured, compared to 'LEGISLATION'.
Values of lexical frequency are derived from large corpora such as the British National Corpus (BNC), a collection of contemporary samples of spoken and written British English that contains a total of 100 million words
Frequency information of content bearing nouns and verbs is derived from their 'lemma' form, which is free from inflection; for example the lemmatised form of 'BLOW', 'BLOWS', 'BLEW' and 'BLOWING' is 'BLOW'.
As such researchers should convert words to their lemma form prior to calculation of these metrics so as not to, for example, underestimate the occurrence of a word in a sample.
NLP-based analytical platforms enable automatic extraction of these properties from a text sample.
The Tool for the Automatic Analysis of Lexical Sophistication
Whilst much of the research utilising this tool has centred on evaluating the proficiency of second language acquisition, the possibilities for applying it to clinical language data are clear.
Coh-Metrix
A more recent study by
No decline over time was found, however, the sample was small with only six participants, and the dementia group was diagnostically mixed, leaving the study greatly under-powered.
There have been fewer large-scale studies of lexical properties of spoken language, which is less conducive to archiving.
Moreover, people tend to produce a much wider range of vocabulary when writing
The UK Prime Minister Harold Wilson was also diagnosed with AD in later life, and
Two important caveats concerning the use of lexical properties are, first, that values (particularly for word frequency) change with fashions in word usage, and secondly that low frequency words are typically under represented
Furthermore, while the discourse of published authors and politicians offers a unique opportunity to analyse language prior to a diagnosis, it cannot be discounted that these individuals may not be representative of the wider population.
The longitudinal nature of these studies does allow characterisation of changes with respect to the individual's baseline, however more recent research has focused on applying computational techniques to new, more diverse datasets; we revisit this in section 8.1 'Availability of large and diverse datasets'.

Grammatical class
Parts of speech provide information about the relative use of grammatical word classes.
Nouns, verbs and adjectives are the most familiar, but the often used Penn Treebank
'A' and 'THE'), conjunctions (e.g.
'AND' or 'BUT') and subcategories of nouns and verbs, which can be used as features in machine learning models (see section 6).
As many as 20% of words can be assigned to more than one class, largely the highest frequency words in a language, leaving 55-67% of words in a text sample ambiguous out of context
Automatic part of speech 'taggers' are built on the principle of a 'sequence labelling problem', and are able to learn features of connected language that give rise to specific tags (classes) using different approaches.
For example, using the Python Natural Language Processing Toolkit (NLTK) tags can be assigned using a machine learning algorithm ('Perceptron tagger'), which learns the context that gives rise to a particular tag from a large corpus, and then applies this knowledge to tag words in a sample
Current tagging approaches can assign a tag to each word of a language sample with around 97% accuracy

Richness
In addition to the lexical properties of individual words, the richness of lexical choices in a sample of discourse may also be informative.
The richness of President Reagan's discourse begins to change prior to his leaving office and being diagnosed with AD, with a decline over time in unique words used, and an increase in non-specific nouns (e.g.
'something') and fillers (e.g.
'um', 'ah) in press conference transcripts.
No change in these measures was detected in the language of his immediate successor George H.W. Bush
The type token ratio (TTR) of a text is a simple measure of the lexical diversity in a sample of text, quantifying the rate of re-use of each unique word in a sample of discourse.
Types are the individual words, while tokens are the instances of types.
For example, the sentence 'I like brown dogs and big dogs' has seven tokens, but only six types (as there are two tokens of the type 'dogs'), giving it a TTR of 0.9.
When calculated over a large window of text, TTR acts as an index of vocabulary size, while TTR at successive windows of text indicates the rate at which words tend to be re-used throughout a sample.
TTR revealed changes over time in the author Iris Murdoch's vocabulary, which appeared to have diminished by the time she started writing her final book, in which she introduced new words at a slower rate than in earlier works
A difference between the first and second halves of the final work of the Dutch author Gerard Reve, also diagnosed with AD in late life, suggested a shrinking vocabulary over the time during which the book was being written
This contrasts with healthy ageing, in which TTR has been found to increase with age, suggesting a more diverse vocabulary across the lifespan
As the computation of TTR includes token counts, samples of different lengths cannot be directly compared.
Many solutions have been suggested, of which the most commonly adopted has been the Moving-Average Type Token Ratio (MATTR)
Windows of varying sizes can be used, and an average calculated.
NLTK includes methods for estimating TTR and MATTR, while the Tool for the Automatic Analysis of Lexical Diversity
When using off-the-shelf NLP tools such as this, users must ensure to understand the computational process from input to output.
For example, what pre-processing is required; are features calculated using all words in a sample, or a sub-section such as content words; and what do values in the output indicate -have they been normalised by token count, for example, or if calculated for a window, as in MATTR, do all samples meet the minimum required length.

Connected language
Inevitably some information is lost through disregarding word order in a bag-of-words approach, but preserving word order allows investigation of syntactic complexity, coherence & cohesion, and entropy.

Syntactic complexity
Syntax refers to the rules which govern arrangement of words in a language to create sentences, such as word order.
Using these rules a sentence can be 'parsed' according to its underlying structure, and the resulting parse visualised and analysed to investigate syntactic complexity.
A syntactic parse tree is defined by clauses and sub-clauses within a sentence, and their syntactic relationships (fig.
There is no single agreed measure of syntactic complexity, and tools are available that calculate a number of metrics.
Lu's L2 Syntactic Complexity Analyzer
The Computerized Linguistic Analysis System (CLAS) evaluates syntactic complexity by calculating three metrics: Yngve and Frazier scores (two approaches to calculating the depth of a syntactic parse tree) (illustrated in fig.
This interestingly coincides with the 'trough' in vocabulary found by
Investigating spoken language,
Tracking syntactic complexity from the MCI stage to moderate AD, later confirmed at post-mortem,
Machine learning approaches that capture syntactic complexity have been found to successfully distinguish both AD and MCI groups from healthy controls

Coherence & cohesion
A semantically coherent piece of discourse follows a theme, or series of themes, which enables the listener (or reader) to follow along.
Incoherent language places a higher cognitive load on the listener,
Cohesion is an objective property of individual words; cohesive devices aid coherence by cueing the listener and helping them connect ideas, such as anaphora -words that refer back to a preceding clause.
Discourse presents a unique opportunity to interrogate these properties
Traditional approaches to scoring coherence and cohesion suggest that these measures may be sensitive to cognitive decline in AD.
However, the use of different scoring methods across studies means that results cannot easily be compared, and may be subject to bias.
The Coh-Metrix platform
Using this tool to analyse narrations of the Cinderella story, patients with mild AD were found to have poor global coherence, while those with MCI did not differ from controls
As Coh-Metrix was originally designed to analyse writing, researchers should take care to remove any fillers or markers in a transcript, such as laughter, prior to analysis.

Entropy & perplexity
In information theory, entropy is used as a measure of the degree of uncertainty within a random variable and is linked to the predictability of a sequence.
A sequence with low entropy has high predictability; when previous values are known, subsequent values can be predicted with more certainty.
Entropy was first applied to language in 1951 by Claude Shannon
Taking each unseen character as a variable, the amount of information inherent in that variable is tied to its predictability when previous characters are known.
In language this will depend on higher order considerations (such as context or grammatical correctness) rather than just the rate of co-occurrence of individual letters.
For example, in the sequence 'The king married the q…' there is 100% probability that the unseen variable is 'u', so its identification does not reduce prior uncertainty.
For the next character in the same sequence, there is less predictability, as the sentence could conceivably be 'The king married the quick-witted woman'.
An estimate of the Shannon entropy of a passage of text can be made by averaging the values associated with every character.
Entropy can also be applied at a more coarse-grained, sentence level of analysis.
Further, Hernández-Domínguez, Ratté, Sierra-Martínez & Roche-Bergua
Closely related to entropy is perplexity, a measure of how accurately the probability distribution of words, word-pairs, word-triplets, etc. (i.e.
n-grams) in a sample of text predict the words that appear in an unseen portion of the same text.
As with entropy, low perplexity indicates high probability of sample prediction, and equates to the number of possible n-grams the model would deem likely
Perplexity decreased across the final three novels of Murdoch's lifetime, from 1987 until her final novel written in 1995, suggesting that the vocabulary of these later works was less diverse.
When analysing only the narrative sections of the novels, the pattern of perplexity showed a steady increase across the lifetime, indicating language growing in complexity across Murdoch's career, before declining.
In a longitudinal analysis of spoken language,
Thus lower perplexity may serve as a useful prognostic indicator of future decline, predicting later severity of cognitive decline, though the sample size was again small, with follow-up data available from only five ADs and 15 MCIs.

Semantics
Semantics of a language sample are concerned with the meaning and ideas the speaker or writer wishes to convey.
Our understanding of semantic memory and language is due in part to an unusual syndrome, semantic dementia (SD), in which specific atrophy of the anterior temporal lobe leads to speech that is fluent but lacking in meaningful concepts (Landin-Romero, Tan, Hodges, & Kumfor, 2016;
AD pathology also gives rise to a semantic impairment, although not specific nor as pronounced as SD
The Computer Language Analysis (CLAN) cross-platform program can be used to analyse semantic content
Originally developed for child language data, CLAN has grown to enable the creation and in-depth analysis of a variety of clinical datasets.
Compared to controls, a sub-group displaying subtle cognitive deficits that did not meet the threshold for MCI, termed 'early MCI' (eMCI), were found to decline faster over time in these semantic features, and measures of fluency such as filled pauses, when describing a picture, suggesting that speech was fluent but lacked specific content.
Interestingly both groups declined in lexical features, but cognitive status was not an indicator of performance.
Reflecting findings of
Standard tests of semantic function, such as picture naming, did not correlate with semantic connected speech measures, suggesting that sampling connected speech -the end product of a number of different cognitive processes -results in a different type of data to stand-alone neuropsychological tests
While this investigation in to early MCI reveals changes in language in a potentially at-risk cohort, the pathway of those diagnosed with eMCI is not yet known, so findings cannot be generalised to early AD.

Latent Semantic Analysis
How do we know what a word means, or the ideas that it represents?
Humans can come to know the meanings of words even without direct sensory exposure to the concepts for which they stand.
'Innatist' philosophers, beginning with Plato, argued that this implies that some knowledge is hardwired into the brain at birth, an idea opposed by Locke and the empiricist school.
Latent Semantic Analysis (LSA)
Beginning with a large contingency table containing all linguistic episodes (typically paragraphs) in a corpus, and the number of times every word type occurs in each, singular value decomposition is used to reduce dimensions of the matrix to those which depend on particular groups of words tending to occur together across contexts.
The output of the process is a high-dimensional vector space of words, where the distance between each word vector, or 'word embedding', is used as a metric of semantic similarity.
For example, the words 'doctor' and 'physician' seldom co-occur, but they often occur in similar contexts as they are close in meaning, leading to their embeddings being close in a semantic space.
This approach has many applications in NLP, such as automatically grouping news articles according to content, regardless of whether the same words appear in each.
In clinical contexts, LSA has been used in a variety of ways to characterise differences and changes in semantic content in clinical populations.
For example, it has been used to characterise coherence of thought to detect the severity of thought disorder in schizophrenia
Combined with neuroimaging, the application of LSA has shed light on the underlying neural systems that support coherent speech in healthy adults
In the AD field, Dunn, Almeida,
Their method had the highest correlation with measures of global cognition, and was not subject to the same floor effects, or potential bias.

Idea density
Idea density is a metric that quantifies how conceptually rich a sample of language is -how many ideas is a person expressing, and how concisely?
To calculate the idea density of a sample, sentences are first segmented in to propositions, before the ratio of propositions to words is calculated, with higher values indicating a greater number of ideas expressed with fewer words.
A lower score could indicate the expression of fewer ideas, or the use of more words to express the same number of ideas
The Computerized Propositional Idea Density Rater tool (CPIDR;
It offers a speech mode for analysis of transcribed speech, which excludes fillers, repetitions and hesitations.
One of the most famous studies investigating language and AD is the Nun Study
A strong and consistent relationship between idea density of autobiographical essays written on entry to a convent and later cognitive function was found; those who displayed lower idea density at an early age were more likely to have poorer performance on neuropsychological tests years later.
Recent computational analysis has replicated these findings in a more representative, yet smaller, AD cohort
Chand, Baynes, Bonnici, & Farias (2012) assert four issues which arise from computational analysis of idea density using CPIDR, such as errors at the tagging stage which impact calculation of propositions, but

Sentiment
The field of NLP has long been concerned with the classification of writing according to the sentiment it expresses, useful for automatically categorising, for example, consumer reviews.
In clinical research this approach has been used to detect depression and neurodegenerative disorders using social media posts
Sentiment can be predicted using machine learning, or using dictionaries of words annotated according to emotional valence, such as Linguistic Inquiry & Word Count (LIWC;
It provides 93 scores relevant to a range of psychological states, personal concerns, and relationship with the past or future, by comparing each word in a sample to its internal dictionaries.
Features relating to time and space were found to be the most important in a study classifying conversations of participants with and without MCI, with 83.33% accuracy (see machine learning section 6), above chance level of 60% given the sample.
This dropped to 76.46% on a sub-set of the data matched for education, suggesting the higher education level in the control group played a role in classification
Limits of using an annotated dictionary approach should be considered: as words are treated as uni-grams (i.e.
without context), negations such as 'I was not feeling happy' cannot be taken in to account

Machine learning
Machine learning is a set of computational techniques that aims to learn patterns in data, and apply what has been learnt to generate successful predictions on new data.
In clinical medicine, for example, the characteristics of disease are learnt from multiple features, and an individual's disease status predicted given these features
Supervised learning is most common, whereby the data used to train and test performance of an algorithm consists of vectors of feature values labelled with their corresponding diagnosis, or 'class'.
In the training phase, an algorithm learns weighted values associated with each feature for the class of interest; features which hold predictive value gain large weights, while features of little or no value are smaller or zero respectively.
In the testing phase, the algorithm predicts class membership for data that was not seen during training as an indication of performance, typically a portion of the same dataset which is held back.
This generalisability to unseen data is key

Performance metrics include total percentage accuracy, sensitivity and specificity, and Receiver
Operating Characteristic Area Under the Curve (ROC-AUC, or AUC), which indicates performance at different thresholds of sensitivity and specificity summarised as a number between zero and one, with 0.5 representing chance level.
Accuracy may not be a useful indicator of performance if classes are imbalanced, or the cost of mislabelling as a false negative or false positive is not equal.
Health datasets are also often small, and in these cases performance can be estimated using k-fold crossvalidation, with available data split into k different folds of training and test sets, and performance averaged across folds.
This can help balance variability in the data set, and detect over-fitting, where a model fits the training data very well, but does not generalise
Using crossvalidation, different model parameters can be tested in order to select the optimal model for good performance on the test set without over-fitting
For example a regularisation parameter introduces a penalty for weights the model learns, ensuring it does not become too complex
In a seminal study
Using cross-validation, a maximum accuracy of 81.92% was achieved using a sub-set of 35 features, selected according to their correlation with the class.
A further factor analysis of the top 50 features found four factors: semantic impairment, acoustic abnormality, syntactic impairment and information impairment, in order of variance in the data explained.
There was no single profile of impairment, suggesting heterogeneity in linguistic decline possibly due to spread of pathology.
Interestingly, values of the semantic and syntactic factors correlated in the control, but not patient, groups, suggesting a decoupling of language abilities in AD
Features of the semantic impairment factor were similar to those which
Deep learning represents a subset of complex algorithms which contain an extra layer, or layers, capable of learning interactions between features and directly from an input, without the need necessarily to first extract features
An AUC of 0.83 for classifying AD, and 0.80 for MCI (both compared to a control group) was achieved, with models with more layers achieving better performance, demonstrating the effectiveness of deep learning in this domain.
However, it is not possible to extract information regarding feature importance from such models, rendering them more of a 'black box' approach
Syntactic and semantic features were found to be most predictive of MMSE score, in keeping with other studies of semantic features in connected speech
Overall performance was best when combining predictions from different classifiers built using connected speech features and other tasks, achieving an accuracy of 84% and AUC of 0.90.
Moving away from group level predictions, towards those at the individual level, will have greater impact for measuring clinical risk, prognosis and treatment response.

Neural word embeddings
Similar to LSA (section 4.1), neural word embeddings represent the meaning of words in a highdimensional vector space, but are built using deep learning.
In an early approach called Word2Vec, a model is trained to predict either a target word in the centre of a window given the context (a continuous bag-of-words model), or the context of a window given the target word (skip-gram model).
The learned weights of this model are used to build a high-dimensional semantic space, in which each word is represented by a unique vector
Word2Vec has been successfully used to measure prose recall in schizophrenia and predict classes of patients and controls
In the GloVe ('global vectors') approach, global cooccurrence statistics across the whole corpus are utilised along with a smaller window looking at context
The Python Gensim library
All, however, entail important preprocessing steps
Using the average word embedding of a sample, representing average 'meaning',
This is supported by
Added to other features, they achieved 86.1% accuracy classifying patients with AD and MCI compared to controls.
Investigating MCI only,
Additional data from patients of another language was more effective than additional data from healthy controls of the same language, with overall accuracy reaching 72% using information content.
Thus, while semantic information captured using neural embeddings alone may not lead to optimal detection of AD, these methods are being utilised in innovative ways to automate steps in analysis and augment data sets.
In a different task,
lacking an organic cause), using conversations with an 'intelligent virtual agent' (IVA) asking similar questions to a Neurologist.
Whilst an important classification, due to the need to make ongoing referral decisions in primary care
There are limitations to the approaches outlined to creating word embeddings, such as an inability to model polysemy, where the same word has multiple meanings.
Newer approaches seek to overcome this issue: ELMo (Embeddings from Language Models), which learns word embeddings from a whole sentence
These newer techniques are performing at state of the art levels in a range of language tasks, and although are yet to be applied to the field of dementia (to our knowledge), may lead to increased accuracy and precision when modelling AD discourse.

Automatic Speech Recognition
To fully automate the process of diagnostic classification and scoring, language samples need to be quickly and accurately transcribed, a goal that can be achieved through ASR, which uses a range of computational methods, including machine learning to automatically generate words from audio recordings, and can circumvent the need for human transcribers, which is costly and unscalable
As current ASR systems are not 100% accurate, work has investigated their utility in clinical fields.
Early studies suggested that its use may negatively impact subsequent machine learning classification tasks, with performance dropping as errors in ASR transcription, measured using the Word Error Rate (WER), increase
Thus both approach and dataset quality may be key, with improvements in these domains leaving researchers with more time and resources to collect data.
Outside of the research setting use of ASR in the dementia field may be problematic as its accuracy is particularly effected by age of voice and frailty.
WERs gradually increase with age
These errors may 'propagate' downstream
Adapting ASR systems for older voices can help to reduce errors:
Given that early detection of AD will rely on ASR capabilities in adult voices, as opposed to older, current systems may be appropriate.

Discussion
We have described the most important advances in NLP and machine learning, and shown how these have stimulated interest in computational studies of the impact of AD on discourse, with research moving towards answering clinically important questions quickly, objectively and with reproducible results.
The tools and approaches available are expanding, with developments in neural approaches opening new pathways for investigation of discourse.
They show potential to be deployed as clinical applications, but they also hold the promise of helping to better understand the underlying mechanisms of AD as they are manifested through the assessment of different components of language.
However, research outlined throughout remains retroactive.
For these tools to be used effectively and implemented into practice, there still remain several practical issues that need to be overcome before the field can progress from research to clinical application, and we outline these below.

Availability of large and diverse datasets
Datasets suitable for NLP analysis are scarce and most often consist of samples of spoken or written English
Many studies have been performed on a small sample of authors, or using the DementiaBank dataset, and thus results may not generalise to other populations, with variation in education, age, and language.
This is particularly problematic for machine learning studies which train and test a model on one dataset, as algorithms can be very 'brittle', with performance dropping when applied to new data, such as in a clinical setting.
To obtain an accurate measure of performance, algorithms could be tested on a separately collected dataset.
While more data for training and testing algorithms within this domain may increase performance, diverse datasets are required for results that will generalise to the clinic; novel methods such as augmenting datasets using other languages are providing promising results
In terms of stimuli, there has been much focus on the cookie theft picture, which may miss important features of different or longer discourse samples,
Longer samples require time consuming and costly transcription, though ASR promises automation of the analytic pipeline.
There is also still relatively little known about how language changes across the lifetime in healthy ageing for the wider population.
Better normative data for specific linguistic features outlined will enable more accurate interpretation of clinical results, with longitudinal studies, such as WRAP which includes language samples

Ethics of data collection & sharing
Collection of the large and diverse datasets required entails ethical guidelines for data collection and storage to ensure participant safety and protection of personal information.
While these constraints do not apply to openly shared data (e.g.
blog posts), diagnoses are less reliable and production conditions unknown.
To achieve the large, diverse datasets necessary to advance the field, sharing of data amongst researchers is crucial.
Fraser, Linz, Lindsay, & König (2019) outline these complex issues in depth, along with examples of good data sharing and recommendations, such as obtaining consent from participants for re-use of data, and considering the type of discourse samples collected, such as avoiding personal histories to maintain privacy.

Diagnostic specificity
One intrinsic problem with detecting AD through discourse is that disease can only be confirmed at post-mortem.
Clinical diagnosis is often difficult; for those diagnosed with AD in life, sensitivity to postmortem thresholds for disease have been found to range from between 70.9% and 87.3%, and specificity between 44.3% and 70.8%
A few studies have utilised data from post-mortem confirmed AD, such as
The increasing availability of brain tissue through brain banking could lead to increased diagnostic specificity not only in computational linguistics, but all forms of clinical research.

Clinical acceptability
Once in the clinic, for a dementia 'flag-raising' tool to be useful its use must be acceptable by both clinicians and patients.
Trust in a tool or approach being used is key, and we break this down to two important factors: interpretability and accountability.
Interpretability involves the level at which a tool's output, such as the decision a patient is at risk of disease, can be explained -how was this decision reached?
How does it map on to clinical understanding of disease mechanisms and symptomatology?
There is usually a trade-off between interpretability and performance; deep learning methods can achieve higher performance than traditional machine learning approaches, but their interpretability is low
Research which attempts to open this black box is gathering pace, but must be a consideration for clinical utility.
In terms of accountability, who is accountable if something goes wrong, such as an error in the system that leads to a false negative, or a data breach?
Clear accountability will help foster trust.
Patients may have additional concerns regarding acceptability, such as around transparency in how their data will be collected and used; whilst there are already laws and frameworks in place for clinical data, the use of technology, for example with remote monitoring, brings new challenges.
The ease of use of a tool, or its intrusiveness, should also be considered.
Some of these questions are starting to be addressed;
Others, such as around accountability, are only now being discussed, and have some way to go before decisions are reached.
Contributions from all 'stake-holders', including researchers, clinicians, patients, health services and commercial enterprises, will likely be required.
The current and future challenges outlined are all linked -good generalisability will foster trust, as an algorithm will not be biased or more unreliable for certain groups, be that gender, ethnicity, or age or socioeconomic status
There is a wider question of whether early detection of disease is beneficial for patients given that there is currently no disease modifying treatment, however, as well as the need to identify those at risk to enable drug development and testing, it is widely considered an acceptable goal, and research has found that patients largely welcome early detection

Conclusion
Undoubtedly NLP and machine learning techniques, and their application to AD, is gathering pace.
The wealth of open source tools should enable greater homogeneity and reproducibility of methods, with researchers able to access tools and share code to study language features objectively, without the necessity for advanced programming skills.
The techniques can provide insights into the underlying neurobiology of language as well as practical tools.
This enables a multidisciplinary field, which benefits from clinical knowledge, although as we have outlined, researchers must understand processes and pitfalls of different approaches.
Newer language models, investigated in larger cohorts, may bring new insights in to early language change in AD and MCI, leading to increased detection of at-risk individuals and optimal monitoring and assessment post-diagnosis.
However, it may be some time before findings have clinical implications, given the challenges yet to overcome.



Figure 1 .
Figure 1.
An example syntactic parse tree.
Parsed using the NLTK Python library.
Syntactic labels: Det = determiner, N = noun, NP = noun phrase, PP = prepositional phase, S = sentence, VP = verb phrase.



Figure 2 .
Figure 2.
An example dependency parse.
Parsed using the spaCy Python library.
Parts of speech: ADP = adposition, DET = determiner, NOUN = noun, PROPN = proper noun, VERB = verb.
Syntactic dependency labels: det = determiner, nsubj = nominal subject, pobj.


An LSA web-based platform developed at CU Boulder (http://lsa.colorado.edu/;
see
A number of metrics are available, including sentence to sentence comparison for measuring coherence.
The aforementioned Coh-Metrix platform outputs eight measures based on latent semantic variables of texts.



Figure 3 .
Figure 3. Example output from CPIDR 3.2 for the sentence 'Mary walked the dog in the park'.
Output shows that it contains seven words and two propositions, giving an ID of 0.286.


the presence of linguistic change, what is the underlying pathology?
Rentoumi et al., (2014) OPTIMA Cookie Theft Computational extraction of linguistic variables 18 pure AD (ADp), 18 mixed AD pathology (ADm) ADp vs ADm: 75% accuracy Question 5: In the presence of cognitive change, what is the degree of linguistic decline?



Table 1 .
NLP tools available for computation of different linguistic features, referenced throughout this review.



performance Question 1: Does this patient have dementia?



Is this patient at risk of developing Alzheimer's disease?



Table 2 .
Studies utilising NLP methods to investigate connected language in AD, organised according to five questions of clinical interest.