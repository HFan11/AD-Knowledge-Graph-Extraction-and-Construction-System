Unsupervised assessment of cognition in the Healthy Brain Project: Implications for web‐based registries of individuals at risk for Alzheimer's disease
Web-based platforms are used increasingly to assess cognitive function in unsupervised settings.
The utility of cognitive data arising from unsupervised assessments remains unclear.
We examined the acceptability, usability, and validity of unsupervised cognitive testing in middle-aged adults enrolled in the Healthy Brain Project.
Methods: A total of 1594 participants completed unsupervised assessments of the Cogstate Brief Battery.
Acceptability was defined by the amount of missing data, and usability by examining error of test performance and the time taken to read task instructions and complete tests (learnability).
Results: Overall, we observed high acceptability (98% complete data) and high usability (95% met criteria for low error rates and high learnability).
Test validity was confirmed by observation of expected inverse relationships between performance and increasing test difficulty and age.
Consideration of test design paired with acceptability and usability criteria can provide valid indices of cognition in the unsupervised settings used to develop registries of individuals at risk for Alzheimer's disease.

INTRODUCTION
Over the past decade, studies of Alzheimer's disease (AD) pathogenesis have examined relationships between cognitive dysfunction and AD biomarkers, which has enabled the characterization of very early stages of the disease (eg, preclinical AD).
2]
Conventional approaches to identifying early AD-related cognitive dysfunction have required individuals to attend medical research facilities at regular (eg, annual) intervals and undergo detailed assessments.
However, such approaches are expensive, time consuming, and can be burdensome to participants.
These limitations have prompted development of cognitive tests relevant to preclinical AD which can be administered remotely and in an unsupervised manner, for example, using online platforms to develop registries of people who may be at increased risk of dementia.
supervised cognitive testing allows for the rapid recruitment of large samples in a cost-effective manner, a reduction in administrator bias, automated scoring by computerized algorithms, and the ability to maximize sample generalizability by facilitating the inclusion of participants who reside in remote locations.
The absence of an assessor to establish rapport may also cause the unsupervised assessment to become impersonal and thereby reduce levels of motivation or engagement leading to sample attrition.
Additionally, it cannot be presumed that cognitive tests applied in unsupervised settings are equivalent psychometrically to the same tests used in supervised settings.
In particular, without a supervisor it is difficult to determine whether poor performance reflects that the individual did not understand or comply with the rules or response requirements of the cognitive test, or whether it reflects a true reduction in the aspect of cognition measured by that test.
Thus, in unsupervised cognitive testing there is a need for indices of performance that may provide guidance on this decision.
Human Computer Interaction (HCI) is a field of research which explores the interaction between humans and computerized technology.
In particular, HCI research looks to study the user's experience (ie, usability and acceptability) of the computerized platform in question.
HCI models, therefore, provide a useful framework for examining the development of unsupervised tests administered remotely using technology.
The HCI concepts of acceptability and usability have been applied in experimental studies of supervised computerized cognitive tests.
9]
HCI usability was operationalized as the amount of time taken to read the test instructions and to complete each test (learnability) and participants' ability to adhere to the requirements of each test (error).
These same concepts can be applied to understand the quality of data from unsupervised cognitive tests administered through web-based platforms in large and unselected samples of individuals at risk of AD.
It is then possible to investigate their validity using standard psychological approaches.
The first aim was to apply an HCI framework to determine the HCI acceptability and usability of unsupervised cognitive tests conducted on participants enrolled in the Healthy Brain Project (HBP), an online study of at-risk middle-aged adults with high rates of first-degree family history of dementia.
The first hypothesis was that unsupervised cognitive tests conducted via a web-based platform would have high HCI acceptability (low rates of missing data), and high HCI usability (high learnability; low error rates).
The second aim was to determine the validity of unsupervised cognitive tests by examining the relationships between performance and test difficulty, and between performance and age.
The second hypothesis was that decreasing performance would be associated with increasing test difficulty, and that participants would show also expected age-related decreases in test performance.
The third aim was to explore the effects of testing environment (eg, home/work alone, home/work with others around, public space), and first-degree family history of dementia on cognitive performance.
The third hypothesis was that participants who complete testing in environments with higher levels of distraction (eg, with others present, or in a public space), and participants with a first-degree family history of dementia, will show worse cognitive performance than those who complete testing in a quiet environment (eg, at home, or alone), and those without a first-degree family history of dementia.

METHODS

Participants
The self-referred to the study.
Further details regarding the inclusion and exclusion criteria and recruitment process for the HBP have been described elsewhere.
As recruitment into the HBP is ongoing, the current report only includes data that have been collected up to the first formal DataFreeze (August 2018).
e HBP platform was designed to allow participants to complete the study questionnaires and surveys over multiple sessions, and in any order.
However, participants were required to complete their cognitive testing in one sitting.
Further, the cognitive tests are administered in a pre-specified order which the participant must follow.
The aim of this approach was to reduce participant burden and to allow participants' maximum flexibility to participate in the study in their own time, while ensuring consistency in the administration of the cognitive tests.
We have previously reported that participants typically complete all HBP assessments across a month.
The HBP was approved by the human research ethics committee of Melbourne Health.

Cognitive tests
Unsupervised cognitive testing was carried out using the Cogstate Brief Battery (CBB) for which instructions and delivery have been modified for online assessment.
The CBB has to be completed on a web browser (Internet Explorer, Google Chrome, or Safari), and as such, participants were required to complete their cognitive testing on a desktop or laptop.
Participants were directed to the CBB platform via the HBP website.
Upon launching the CBB platform, participants were required to complete the test battery in one sitting.
The approximate completion time for the entire CBB is 20 minutes.
CBB tests are downloaded onto the browser, completed locally, and then uploaded.
This allowed for the minimization of the impact of internet connectivity on test performance.
The CBB has a game-like interface which uses playing card stimuli and requires participants to provide "Yes"
or "No" responses.
The CBB consists of four tests: Detection (DET), Identification (IDN), One Card Learning (OCL), and One-Back (OBK).
These tests have been described in detail previously,
Briefly, DET assesses psychomotor function, and IDN assesses visual attention.
The primary outcome for both DET and IDN was reaction time in milliseconds (speed), which was normalized using a log 10 transformation.
OCL assesses visual learning, and OBK assesses working memory and attention.
The primary outcome measure for OCL and OBK was proportion of correct answers (accuracy), which was normalized using an arcsine square-root transformation.
e four different tests in the CBB were designed to use identical which requires a single button press in response to a stimulus change.
The IDN test is presented second and this requires that in addition to detecting the stimulus change, individuals must also discriminate stimuli based on their color (red or black).
The OBK is the third test and requires that discrimination decisions be based on color and number information held in working memory.
The OCL is the final and most difficult test on the basis that it requires decisions be based on stimulus information learned and retained throughout the test itself.
Consistent with Sternberg additive factors logic,
Such relationships are observed reliably in supervised studies of cognitively normal younger
As such, the examination of the same test difficulty/performance relationships would provide a sound criterion for determining test validity in unsupervised performance.

Testing environment survey
Participants were asked to indicate (1) never, (2) rarely, (3) often, or (4)   all the time to the following questions: type of environment typically (but are quiet), (

Data analysis
All analyses were conducted using R Statistical Computing Software (R version 3.5.0).
Given the large number of comparisons, statistical significance was adjusted using the false-discovery rate (FDR) correction.
r each cognitive outcome, HCI acceptability was defined as Finally, in a subgroup of participants who also responded to a survey of their testing environment (n = 827), we explored whether cognitive performance varied across different testing environments.
To achieve this, we grouped participants into (1) "alone" (ie, those who responded "all the time" or "often" to completing tests at home or at work without other people present), (

RESULTS

Sample overview
The demographic characteristics of our sample are provided in Table
Older age groups were associated with lower levels of education, lower annual income, and fewer symptoms of depression and anxiety.
Older age groups were also more likely to report a first-degree family history of dementia, and postcodes from rural/regional locations.

HCI acceptability of unsupervised cognitive testing
Analyses of HCI acceptability indicated that 30 (1.9%) cases of missing data occurred for DET, and 10 (0.6%) cases of missing data occurred for IDN.
No missing data occurred for the OCL or OBK.

HCI usability of unsupervised cognitive testing
Analyses of HCI error indicated that the proportion of completed cognitive tests that did not exceed pre-specified error criteria were 93.4% for DET (n = 1488), 89.5% for IDN (n = 1427), 94.6% for OCL (n = 1508), and 94.7% for OBK (n = 1510).
When this analysis was considered for the four cognitive tests simultaneously, 19.6% (n = 313) of participants exceeded error criteria for one cognitive test, 4.6% (n = 73) for two cognitive tests, 2.0% (n = 32) for three cognitive tests, and 1.6%
(n = 25) on all four cognitive tests.
Examination of the demographic characteristics of participants who did and did not meet HCI error criteria indicated that participants who did not meet error criteria on at least two cognitive tests were more likely to be male and required significantly longer time to complete all cognitive tests (Table
Results from the analysis of HCI learnability (ie, time taken to read instructions and complete tests) are summarized in Table
As a significant difference in the amount of time taken to complete tests was observed between participants who passed or failed HCI error We observed age-related increases in the amount of time taken to read test instructions, but only for the DET test (Table
Similarly, we observed age-related increases in the amount of time taken to complete all cognitive tests.
Over 90% of participants met HCI learnability criteria for the test instruction screen, and for time taken to complete each cognitive test, with no differences across age groups (Table

Validity of unsupervised cognitive testing: relationship with test difficulty and age
For participants whose data satisfied HCI acceptability and HCI usability criteria, group mean (SD) performance on each cognitive test is summarized in Table
As predicted, speed of performance varied as a function of test difficulty.
Paired samples t tests of the overall group performance indicated that average performance on the OCL test was significantly slower than on the OBK test, t(1520) = 56.92,
P < .001,
which in turn was significantly slower than the IDN test, t(1520) = 101.22,
P < .001,
which in turn was significantly slower than the DET test, t(1520) = 75.006,
P < .001.
Similarly, accuracy of performance on the DET test was significantly better than the IDN test, t(1520) = 12.84, P < .001,
which in turn was significantly than the OBK test, t(1520) = 5.62, P < .001,
and in turn was significantly between than the OCL test, t(1520) = 87.55,
P < .001
(Table
After adjusting for participants' years of education, we observed that speed of performance across most cognitive tests was significantly slower in older age groups (Table
When participants' age was considered continuously (and adjusting for years of education), speed of performance on all tests was significantly slower with older age; DET  tests, although these associations were, by convention, very small in magnitude.

Effect of testing environment, and first-degree family history of dementia on cognitive performance
A subsample of participants (n = 827; 62%) who completed cognitive tests also completed a survey of their usual testing environment.
without other people present (41.1%) or at home/work where other people are present but are quiet (52.4%).
Participants in the "with others (noisy)" group were significantly younger than those in the "with others (quiet)" group (P = .011,
d = 0.37), who were in turn younger than participants in the "alone" group (P < .001,
d = 0.26).
Error and learnability did not vary significantly across each of the testing environments (Table
The effect of testing environment on the primary outcome measure of each cognitive test, after accounting for the effects of age and education, are summarized in Table
Age-and education-adjusted performance mean (SD) of each testing environment group are also provided in Table
We observed no overall effect of testing environment on any cognitive outcome measure.
However, post-hoc comparison suggests that participants in the "with others (quiet)" group performed significantly slower than those in the "alone" group on the IDN test, but the magnitude of difference was very small, d(95%

DISCUSSION
The aim of this study was to determine the acceptability and usability of unsupervised cognitive testing.
With those characteristics established, we also aimed to determine the validity of unsupervised cognitive testing in middle-aged adults enrolled in a large online study of AD risk.
A large group of adults from metropolitan, regional, and rural Australia enrolled in the HBP completed an online battery of computerized cognitive tests.
In this group, we showed that with appropriate design, a battery of unsupervised cognitive tests showed high acceptability and usability.
Low rates of missing data were observed (1.9%), which occurred only for the first and second tests in the battery.
This pattern suggests that missing data are more likely to occur in earlier tasks, when participants are less familiar with the battery, and indicates that as participants became more comfortable with the battery as the tests progressed they were able to complete the battery successfully.
The current study sample also demonstrated high levels of understanding and adherence with the rules and requirements of each test, as measured by the extent to which performance on each test fell within expected normative limits.
Limits of accuracy of performance were obtained from normative data for the same tests administered to healthy adults in supervised settings.
performance is well known in the psychological literature and has been observed many times in the context of supervised assessments using these same cognitive tests.
9]
The second aspect of validity demonstrated was that older age was associated with slower reaction time on all tests, and with poorer accuracy of performance on the OBK and OCL tests.
Age-related decline in cognition, even in the absence of neurodegenerative disease, has been well described,
While these effects are small, they do indicate that data arising from the application of these tests in unsupervised settings did retain their validity.
Finally, in a subset of participants, we sought to explore the impact of participants' self-reported testing environment on cognitive performance.
Of the 1594 participants who completed cognitive testing, ∼65% (n = 827) completed a survey of their usual testing environment (defined as either: alone; with others [quiet]; with others [noisy]).
When accounting for the effects of age, no general influence of testing environment was found for any of the cognitive outcome measures in this study, except for a small (d = 0.14) effect of reduced speed of performance in those who completed testing in the "alone" group compared to "with others [quiet]."
These findings contrast with those of the existing literature, which suggests performance on unsupervised neuropsychological tests may be particularly sensitive to test environment.
r example, background music and noise has been reported to impair performance on cognitive tests.
When considered along with the observation that aspects of error and learnability did not vary as a result of testing environment, our findings do provide further support for the use of the CBB in unsupervised cognitive testing.
However, future research comparing the performance of the same individual across different testing environments in both unsupervised and supervised contexts is required to further support this conclusion.
An important limitation to our study is that it is cross-sectional in design, and as such, information about participant retention and attrition were not available.
Additionally, as this study was administered completely online, we did not obtain any data related to the supervised and "alone" [n = 340] groups) and that we did not experimentally determine performance of the same individual in different testing environments.
It should, however, be noted that the testing environment survey was designed to reflect the flexibility of the HBP testing schedule (ie, participants were not required to complete all surveys in a single sitting).
Consequently, there may be some disconnect between the actual environment in which the CBB was completed and where the participant completed the majority of their assessments.
Finally, it is important to note that the HBP does not randomly sample the population.
Family history was used as a proxy for AD or dementia risk, and while having a family history is not a very strong predictor of AD, this strategy has resulted in higher proportions of apolipoprotein E ε4 carriers in the study sample compared to the general population (35% vs 18%).
ile we have established the internal validity of these tests, we need to now determine the external validity by using an established criterion for abnormality.
For example, future studies will need to examine the acceptability, usability, and validity of performance of individuals with clinically diagnosed mild cognitive impairment (MCI).
Recently, poorer performance across all CBB tests was observed in self-reported MCI patients and self-reported AD patients compared to self-reported healthy controls who completed unsupervised testing.
While promising, additional studies are needed to determine the utility of unsupervised cognitive tests in detecting cognitive impairment in clinically confirmed MCI and AD patients.
These caveats notwithstanding, the data in this study show how the HCI approach can provide a suitable foundation for development and refinement of unsupervised cognitive tests.
With acceptability and usability, we showed that psychometric characteristics of cognitive test data generated in unsupervised contexts can be challenged using conventional psychometric approaches.
We also examined the validity of unsupervised cognitive tests by determining the extent to which the expected effects of age and test difficulty manifest in performance.
However, other psychometric approaches such as examination of factor structure, differential item function, as well as the more crucial characteristics such as sensitivity to AD related risk factors or to AD related cognitive change, can also be applied to understand data generated in unsupervised settings.
With these factors considered, the data collected in this study do appear to retain similar psychometric characteristics as those collected from supervised testing of the same tests.
As such, these results support the acceptability, usability, and validity of the CBB in the unsupervised assessment of cognition in individuals at risk of dementia.
The approach used here can also be applied to other cognitive tests and surveys administered via online or web-based platforms for the unsupervised assessment of individuals with self-reported cognitive impairment


sample consisted of 1594 participants who had completed unsupervised cognitive testing as part of their enrollment in the HBP (healthybrainproject.org.au).
Participants were aged 40 to 65 years, lived in the community, and self-reported family history of dementia.
Participants were excluded from enrolment in HBP if they self-reported any of the following: history of major traumatic brain injury; diagnosis of AD, Parkinson's disease, Lewy body dementia, or any other type of dementia; previous use of medications for the treatment of AD; current use of narcotics or antipsychotic medications; uncontrolled major depression, schizophrenia, or another Axis I psychiatric disorder described in the Diagnostic and Statistical Manual of Mental Disorders-Fourth Edition (DSM-IV) within the past year; or a history of alcohol or substance abuse or dependence within the past two years.
Participants were recruited through a series of media appeals, community-based and scientific organizations (eg, Dementia Australia and the Florey Institute of Neuroscience and Mental Health), traditional word of mouth, and social media.
Participants



1
Example of training procedure interface for the Identification (IDN) test in the Cogstate Brief Battery (CBB) in while doing the tests and surveys on the HBP, (1) at home without other people present, (2) at home with other people present (but are quiet), (3) at home with other people present (but are noisy), (4) at work without other people present, (5) at work with other people present


(β[standard error (SE)] = 0.19[0.03],
P < .001),
IDN (β[SE] = 0.23[0.03],
TA B L E 4 Average speed and accuracy of performance for each cognitive test



(
in-person) administration of the CBB.
A direct comparison of unsupervised and supervised CBB testing may have strengthened the impact of the findings of this study by providing further information regarding the acceptability and validity of unsupervised assessment.
We also acknowledge that our comparison of performance across varied testing environments may be limited by both power (large differences in sample size between the "noisy" [n = 54], "with others [quiet]" [n = 433]


All values for continuous variables (age, education, annual income, HADS depression, HADS anxiety, and no. of days between enrollment and test completion) presented as mean (standard deviation); chi-square was used to test differences between groups for categorical variables, and analysis of variance was used to test differences between groups for continuous variables; FDR corrections have been applied to all P values.
Abbreviations: FDR, false-discovery rate; HADS, Hospital Anxiety and Depression scale.
Demographic characteristics of overall sample, and of participants who did and did not meet error criteria on at least two cognitive tests Chi-square was used to test differences between groups for categorical variables, and analysis of variance was used to test differences between groups for continuous variables; FDR corrections have been applied to all P values.
Amount of time spent on instruction screen, and time taken to complete each cognitive test


FDR corrections have been applied to all P values; all values have been adjusted for years of education; P 1 = 49 to 49 years versus 50 to 59 years; P 2 = 40 to 49 years versus 60 to 70 years.
Abbreviations: DET, Detection test; FDR, false-discovery rate; IDN, Identification test; OCL, One Card Learning test; OBK, One-Back test.
Demographic characteristics of participants who completed tests in each testing environment FDR corrections have been applied to all P values.
Abbreviations: CBB, Cogstate Brief Battery; DET, Detection test; FDR, false-discovery rate; HADS, Hospital Anxiety and Depression scale; HCI, Human computer interaction; IDN, Identification test; OBK, One-Back test; OCL, One Card Learning test.



Table 5
provides the demographic characteristics of this subset of participants.
Most participants completed cognitive tests at home/work TA B L E 6 Effect of testing environment, and first-degree family history of dementia, on the primary outcome of each cognitive test



Table 3 )
. As expected, the time required to complete each test varied with test difficulty and length, and this time requirement increased slightly with age.
Together, these data suggest that the unsupervised cognitive tests applied in the HBP are acceptable and usable in the samples sought.
They also provide a guide for the development and application of automated analyses to identify low quality test data and differentiate this from data that reflects true cognitive performance.