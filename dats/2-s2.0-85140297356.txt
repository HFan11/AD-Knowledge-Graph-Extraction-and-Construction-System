A deep learning model for detection of Alzheimer's disease based on retinal photographs: a retrospective, multicentre case-control study
There is no simple model to screen for Alzheimer's disease, partly because the diagnosis of Alzheimer's disease itself is complex-typically involving expensive and sometimes invasive tests not commonly available outside highly specialised clinical settings.
We aimed to develop a deep learning algorithm that could use retinal photographs alone, which is the most common method of non-invasive imaging the retina to detect Alzheimer's disease-dementia.
In this retrospective, multicentre case-control study, we trained, validated, and tested a deep learning algorithm to detect Alzheimer's disease-dementia from retinal photographs using retrospectively collected data from 11 studies that recruited patients with Alzheimer's disease-dementia and people without disease from different countries.
Our main aim was to develop a bilateral model to detect Alzheimer's disease-dementia from retinal photographs alone.
We designed and internally validated the bilateral deep learning model using retinal photographs from six studies.
We used the EfficientNet-b2 network as the backbone of the model to extract features from the images.
Integrated features from four retinal photographs (optic nerve head-centred and macula-centred fields from both eyes) for each individual were used to develop supervised deep learning models and equip the network with unsupervised domain adaptation technique, to address dataset discrepancy between the different studies.
We tested the trained model using five other studies, three of which used PET as a biomarker of significant amyloid β burden (testing the deep learning model between amyloid β positive vs amyloid β negative).
Findings 12 949 retinal photographs from 648 patients with Alzheimer's disease and 3240 people without the disease were used to train, validate, and test the deep learning model.
In the internal validation dataset, the deep learning model had 83•6% (SD 2•5) accuracy, 93•2% (SD 2•2) sensitivity, 82•0% (SD 3•1) specificity, and an area under the receiver operating characteristic curve (AUROC) of 0•93 (0•01) for detecting Alzheimer's disease-dementia.
In the testing datasets, the bilateral deep learning model had accuracies ranging from 79•6% (SD 15•5) to 92•1% (11•4) and AUROCs ranging from 0•73 (SD 0•24) to 0•91 (0•10).
In the datasets with data on PET, the model was able to differentiate between participants who were amyloid β positive and those who were amyloid β negative: accuracies ranged from 80•6 (SD 13•4%) to 89•3 (13•7%) and AUROC ranged from 0•68 (SD 0•24) to 0•86 (0•16).
In subgroup analyses, the discriminative performance of the model was improved in patients with eye disease (accuracy 89•6% [SD 12•5%]) versus those without eye disease (71•7% [11•6%]) and patients with diabetes (81•9% [SD 20•3%]) versus those without the disease (72•4% [11•7%]).
Interpretation A retinal photograph-based deep learning algorithm can detect Alzheimer's disease with good accuracy, showing its potential for screening Alzheimer's disease in a community setting.

Introduction
Alzheimer's disease, the most common form of dementia, is a global public health problem.
For example, biomarkers of amyloid β and phosphorylated tau measured through cerebrospinal fluid assessments, PET scans, and plasma assays are helpful for Alzheimer's disease diagnosis, but these tests are not suitable for screening possible Alzheimer's disease in primary care or community settings.
Of note, because Alzheimer's disease treatment is available,
The retina, a neurosensory layered tissue lining the back of the eye and directly connected to the brain via the optic nerve, has long been considered a platform to study disorders in the CNS because it is an accessible extension of the brain in terms of embryology, anatomy, and physiology.
Retinal changes in Alzheimer's disease have been shown in post-mortem histopathological studies.
This concept is supported by clinical studies showing a range of retinal changes in patients with Alzheimer's disease, such as changes in the retinal vasculature (eg, vessel calibre and retinopathy signs), the optic nerve, and the retinal nerve fibre layer.
These features can be non-invasively imaged using digital retinal photography, which is now widely available at a low cost in primary care optometry and community settings.
Artificial intelligence (AI), particularly deep learning, allows algorithms to extract both known and unknown features from images for accurate detection of a condition, without the need for manual identification of specific features.
Deep learning has been applied to retinal photographs for detecting various ophthalmic diseases (such as diabetic retinopathy,
Furthermore, deep learning approaches can also detect systemic diseases based on retinal photographs (eg, systemic biomarkers,
However, the role of deep learning approaches in detecting Alzheimer's disease from retinal photographs has yet to be determined.
We aimed to develop a novel deep learning algorithm for automated detection of Alzheimer's disease-dementia from retinal photographs alone to determine its possible use for Alzheimer's disease screening.
To address this, we trained, validated, and tested the deep learning models using retinal photographs from 11 clinical studies.
We also tested the ability of our deep learning model to differentiate patients who were amyloid β positive from those who were amyloid β negative.

Methods

Study design and participants
In this retrospective, multicentre case-control study, we trained, validated, and tested a deep learning model for detecting Alzheimer's disease from retrospectively collected retinal photographs from 648 patients with Alzheimer's disease and 3240 patients who did not have the disease.
Our study included 11 clinical studies and was done at eight centres in four countries (Hong Kong Special Administrative Region, China, Singapore, the UK, and the USA; appendix pp 3-7).
The inclusion and exclusion criteria for patients in each of the 11 studies are reported in the appendix (pp 3-7).
For all participants, we

Research in context
Evidence before this study We searched PubMed for studies of deep learning-based Alzheimer's disease detection from retinal photographs published from the inception of the database to Jan 31, 2022.
We used the terms "deep learning" OR "machine learning" AND "Alzheimer's disease" AND "retinal photographs".
The references of identified studies were also reviewed.
We found a study that developed a deep learning system to predict Alzheimer's disease using images and measurements from multiple ocular imaging modalities (optical coherence tomography, optical coherence tomography-angiography, ultra-widefield retinal photography, and retinal autofluorescence) and patient data.
Another study proposed a machine learning model to classify Alzheimer's disease from retinal vasculature.
These models were trained with a small amount of data and without external testing.
Deep learning models trained with large sample sizes of retinal photographs and validated with testing datasets from different populations and testing using biomarkers of Alzheimer's disease are warranted.

Added value of this study
We trained, validated, and tested a deep learning algorithm to detect Alzheimer's disease based on retinal photographs using data from 648 patients with Alzheimer's disease and 3240 individuals without the disease from 11 multicentre clinical studies in different countries.
To the best of our knowledge, our study included the largest sample size and the most comprehensive metadata of patients with Alzheimer's disease for deep learning model development.
We used advanced deep learning techniques (eg, unsupervised domain adaptation and feature fusion) to address two challenges:
(1) data distribution discrepancy between training and validation and testing datasets and (2) individuals who have multiple retinal photographs from each visit, including optic nerve head-centred and macula-centred images of both eyes.
We also proposed deep learning models with different architecture for a practical application.
For example, we trained a unilateral model that can provide an Alzheimer's disease detection outcome when only photographs of one eye are available, covering a common scenario in the real world.
In addition to Alzheimer's disease-dementia detection, we have tested our model in multicentre datasets from different regions and countries (Hong Kong Special Administrative Region, China, Singapore, and the USA) with biomarkers of amyloid β.

Implications of all the available evidence
This is a proof-of-concept study to test a retinal photographbased deep learning algorithm for Alzheimer's disease detection.
We believe that after validation, testing, and integration with deep learning pipeline, our model can be implemented for Alzheimer's disease screening, leveraging community eye-care infrastructure.
See Online for appendix used four retinal photographs (optic nerve head-centred and macula-centred images from both eyes) for the model development.
This multicentre study was approved by the human ethics boards of the Joint Chinese University of Hong Kong-New Territories East Cluster Clinical Research Ethics Committee, Hong Kong Special Administrative Region, China, and local research ethics committees in each centre.
The 11 studies used to generate the test populations were all done according to the Declaration of Helsinki, with written informed consent obtained from each participant or their guardians.
The STARD guideline was used for reporting in the current study.

Procedures
The main aim of this study was to develop a bilateral deep learning model that outputted participant-level detection results (ie, Alzheimer's disease-dementia or no dementia) accounting for Alzheimer's disease features from optic nerve head-centred and macula-centred images from both eyes.
We used retinal photographs from six studies with labels of either Alzheimer's diseasedementia or no dementia as primary datasets (ie, source domain; primary 1-6; appendix pp 3-5) for the development and internal validation of the deep learning model.
We tested the trained deep learning models with five non-overlapping studies that had labels of Alzheimer's disease-dementia or no dementia (ie, target domain; testing datasets 1-5; appendix pp 5-7).
The image quality was labelled by three trained human graders (ARR, VTTC, and KS).
Only gradable retinal photographs were used.
If more than 25% of the peripheral area of the retina was unobservable due to artifacts, including the presence of foreign objects, out-of-focus imaging, blurring, and extreme illumination conditions and if the centre region of the retina had significant artifacts that would affect analysis, the photograph was considered ungradable.
The inter-grader reliability was high, with Cohen's κ coefficients ranging from 0•868 to 0•925.
If grader 2 (VTTC) and grader 3 (KS) could not make a decision as to whether an image should be included (eg, retinal photographs with borderline quality), the senior grader (grader 1 [ARR]) made final decisions.
The labelling of Alzheimer's disease-dementia in all studies followed the Diagnostic and Statistical Manual of Mental Disorders, 4th edition, criteria for dementia syndrome (Alzheimer's type) and National Institute of Neurological and Communicative Disorders and Stroke and the Alzheimer's Disease and Related Disorders Association criteria for probable or possible Alzheimer's disease.
Retinal photographs were labelled as no dementia when the participant had no objective cognitive impairment evident in the neuropsychological assessments and no history of neurodegenerative diseases.
Three testing sets (testing set 1-3; appendix p 6) also included data from amyloid-PET scan examinations following intravenous 11C-Pittsburgh compound B to quantify amyloid β deposition from a series of brain regions.
The retinal photographs with amyloid-PET scan available were additionally labelled as either amyloid β positive or amyloid β negative based solely on the standardised uptake value ratio with reference to the locally validated cutoff value, regardless of their clinical diagnosis.
The details of the primary and testing datasets are described in the appendix (pp 3-7).
Because the labelling input and classification output were dependent on the individual participant rather than the image, the deep learning model was designed to integrate features of Alzheimer's disease from four retinal photographs from each participant (ie, both optic nerve head-centred and macula-centred fields from both eyes).
The datasets were split at a participant level to prevent information leakage and performance over estimation.
Our method consisted of four phases.
In the first phase, we designed a basic model, using EfficientNet-b2,
We then proposed a bilateral model based on four retinal photographs, which learned Alzheimer's disease-related features from optic nerve head-centred and macula-centred retinal photographs from both eyes (figure A).
Specifically, we designed an adaptative feature fusion technique to integrate the extracted information from multiple retinal photographs from both eyes.
The bilateral model (main aim of this study) outputted participant-level detection results (ie, Alzheimer's disease-dementia or no dementia) accounting for Alzheimer's disease features from both eyes.
If multiple paired images were available for the same individual, we sorted the predictive values for all the paired images and used the image with the median value for the final participant-level prediction.
In the second phase, we developed a unilateral model for single eye analysis because individuals might have an ungradable retinal photograph from one eye (eg, due to severe cataract; figure B).
In the third phase, we trained a hybrid model that could consider risk factors of Alzheimer's disease (ie, age, gender, and presence or absence of hypertension and diabetes; figure C).
Finally, we trained a risk factors alone model for Alzheimer's disease prediction (appendix pp 23-25) for comparison with the bilateral model.
We used unsupervised domain adaptation with domainspecific batch normalisation to address data heterogeneity and domain shift problems and to improve the model performance.
Unsupervised domain adaptation is a type of learning framework that can transfer knowledge learned from a larger number of annotated training data in the source domains to target domains with unlabelled data only.
Domain-specific batch normalisation is a building block for deep neural networks for which the source domain and the target domain datasets have their own separate batch normalisation layer for training and

Primary training and validation datasets
extraction of hyper-parameters.
This design addressed characteristics specific to each domain that are not compatible within a single model while retaining domaininvariant infor mation that is common to all domains.
In brief, the labelled source domain dataset was first used for training in a supervised way to generate an unsupervised domain adaption network.
This unsupervised domain adaptation network was then used to generate pseudo-labels for unlabelled data in the target domain.
The final classification network was subsequently trained with full supervision using labelled data from the source domain and pseudo-labelled data from the target domains.
Through the fusion of the domain-independent and domain-dependent knowledge learning, the deep learning models could transfer discriminative features from the labelled source domain to the unlabelled target domain (ie, domain adaptation) and improve the classification performance on the target domain.
Due to poor model transfer capability of the domain adaptationbased method, we trained one model for each testing dataset to obtain the information, such as image style distribution of unlabelled testing datasets.
Furthermore, to better understand discriminative features between patients with Alzheimer's diseasedementia and participants without the disease, we used Gradient-weighted Class Activation Mapping (ie, heatmap) to visualise the features extracted from the last convolutional layer.
Details of the network architecture, training details, and objective functions were described in the appendix (pp 8-15).

Statistical analysis
We used the testing datasets to evaluate the model performance at a participant level on three aspects: clinically diagnosed Alzheimer's disease-dementia versus no dementia, individuals who were amyloid β positive versus individuals who were amyloid β negative, and individuals who had clinically diagnosed Alzheimer's disease-dementia and were amyloid β positive versus those who had no cognitive impairment and were amyloid β negative.
Models were evaluated based on the following metrics from the five-fold cross validation: the area under the receiver operating characteristic curve (AUROC) and values for accuracy, sensitivity, and specificity for which the cutoff point was the largest Youden Index in each dataset.
In subgroup analyses, we combined the testing 1-3 datasets and stratified individuals on the basis of the presence of eye disease from retinal photographs and diabetes diagnosis status to evaluate discriminative performance.
The performance of the unilateral model was also compared between right eyes and left eyes.

Role of the funding source
The funder had no role in study design, data collection, data analysis, data interpretation, writing of the report, and decision to submit the paper for publication.
In the internal validation dataset, the bilateral model had 83•6% (SD 2•5) accuracy, 93•2% (SD 2•2) sensitivity, 82•0% (SD 3•1) specificity, and an AUROC of 0•93 (0•01) for detection of Alzheimer's disease-dementia.
For differen tiation between patients with Alzheimer's diseasedementia and participants who did not have the disease, both bilateral and unilateral models had accuracies of more than 83% and AUROCs of more than 0•9 in internal validation (table
During testing, the bilateral model had accuracies ranging from 79•6% (SD 15•5) to 92•1% (11•4) and AUROCs ranging from 0•73 (SD 0•24) to 0•91 (0•10; table
In the three testing datasets with known amyloid β status from PET, the bilateral model was able to differentiate between those who were amyloid β positive and those who were amyloid β negative with an accuracy ranging from 80•6% (SD 13•4) to 89•3% (13•7), and an AUROC ranging from 0•68 (SD 0•24) to 0•86 (0•16; table
Heatmaps differentiating true-positive and truenegative examples are reported in the appendix (pp 19-20).
The performance of the bilateral model was better than that of the unilateral model in the testing (table
The performance of the unilateral model was largely similar between right eyes and left eyes (appendix p 22).

Primary training and validation datasets
In the subgroup analysis, the ability of the model to differentiate between people with Alzheimer's diseasedementia and those without the disease and those who were amyloid β positive from those who were amyloid β negative was improved in patients with concomitant eye disease (accuracy 89•6% [SD 12•5%]) versus those without eye disease (71•7% [11•6%]; table
Of note, the model performance was maintained when risk factors of Alzheimer's disease (ie, age, gender, and presence of hypertension and diabetes) were included in the model (hybrid model; appendix pp 23-25).
Except for the testing set 5, which had a similar performance, the bilateral model had higher accuracy than the risk factors alone model (appendix pp 23-24).
Compared with the Hong Kong version of the Montreal Cognitive Assessment for Alzheimer's disease-dementia detection in a community-based cohort, our bilateral model's assessment of testing set 5 had higher sensitivity (100% vs 50%) and a higher AUROC (0•91 vs 0•75; appendix p 18).
Unsupervised domain adaptation with domain-specific batch normalisation was used in the testing datasets to address the issue of data heterogeneity and domain shift problems.
After domain adaptation, the model performance was generally improved, suggesting that the model also learned discriminative features from the source domain for Alzheimer's disease detection (appendix p 26).

Discussion
In this study, we developed, validated, and tested a novel, retinal photograph-based deep learning algorithm to detect individuals with Alzheimer's disease, using an unsupervised domain adaptation deep learning technique to improve its generalisability.
Our deep Data are mean (SD).
Five-fold cross-validation method was applied in each testing dataset.
AUROC=area under the receiver operating characteristic curve.

Table 2: The participant-level performance of the deep learning bilateral model and unilateral model in the internal validation and the testing datasets
learning algorithm showed consistently accurate performance for differentiating between patients with Alzheimer's disease-dementia and individuals with no dementia.
In particular, the performance was similar for differentiating between people who were amyloid β positive from those who were amyloid β negative.
In addition, our deep learning algorithm had good performance in the presence of concomitant eye diseases (eg, age-related macular degeneration), thus allowing screening in optometry and ophthalmology settings.
To the best of our knowledge, this is the first deep learning model to detect Alzheimer's disease from retinal photographs alone.
Wisely and colleagues
Although their results were promising and provided proof-of-concept data on using AI to interpret retinal images for detection of Alzheimer's disease, their findings were based on a small sample of 159 individuals, of whom 36 (23%) had Alzheimer's disease, and required multiple specialised imaging modalities, which might not be feasible in a primary care or community setting.
By contrast, our algorithm could predict Alzheimer's disease based on retinal photographs only, thus improving the efficiency and potential cost-effectiveness of the algorithm.
Our algorithm was also developed with two advanced deep learning techniques: unsupervised domain adaptation and feature fusion.
The use of the two techniques addresses two significant challenges: (1) data distribution discrepancy between training and validation and testing datasets, and (2) the integration from multiple optic nerve head-centred and macula-centred retinal photographs from both eyes.
With this deep learning architecture, our algorithm could be transferrable to a new centre without developing a new deep learning model.
Retrospective data can be collected from this specific centre for unsupervised domain adaptation, and the model can subsequently be refined to keep the deep learning model up to date.
To increase applicability, we intentionally included retinal photographs with concomitant eye disease in the training dataset because age-associated eye conditions (eg, age-related macular degeneration and glaucoma) are common in people older than 60 years.
Meanwhile, excluding eyes with these conditions might also introduce selection bias because studies have shown the patients with Alzheimer's disease are more likely to have age-associated macular degeneration and glaucoma.
f note, our deep learning algorithm retained a robust ability to differentiate between people who had and did not have Alzheimer's disease, even in the presence of concomitant eye diseases.
These findings suggest that Alzheimer's disease has unique retinal features that are distinguishable from other eye diseases.
Furthermore, patients with type 2 diabetes are at higher risk of cognitive impairment.
Our deep learning algorithm performed well without significant interference from concomitant diabetes, suggesting its similarity with deep learningbased diabetic retinopathy screening.
However, the performance of the model in participants without eye disease dropped.
Although we do not have a definitive explanation for this observation, it is possible that an overlap in pathophysiological features shared between Alzheimer's disease and eye diseases might enhance the identification of Alzheimer's disease-associated features from retinal imaging, but validation is warranted.
We developed a supplementary unilateral model, which can estimate the risk of Alzheimer's disease based on retinal photographs from a single eye.
A unilateral model is essential for community screening of Alzheimer's disease because retinal photograph of one eye might not be assessable due to media opacity (eg, cataract).
Our results suggest that the unilateral model can also reliably predict Alzheimer's disease-dementia based on unilateral retinal photographs.
Our proposed retinal photograph-based deep learning model provides a proof-of-concept solution to address the current gap in Alzheimer's disease screening, in which under-diagnosis of dementia is highly prevalent.
Early diagnosis of Alzheimer's disease relies on a complex series of cognitive tests, clinical assessments, supportive evidence from neuroimaging (eg, PET), and cerebrospinal fluid biomarker evidence, with the definitive diagnosis only confirmed post mortem.
Therefore, patients with Alzheimer's disease are usually diagnosed late after the onset of debilitating dementia when there has already been extensive brain neurodegeneration that might not be amenable to any disease-modifying treatment.
Strengths of this include a diverse clinical sample, with datasets from multiethnic, multicountry cohorts and in different clinical settings.
Our algorithm was also validated in five testing datasets, three of which included amyloid-PET scan.
Furthermore, we used unsupervised domain adaptation with domain-specific batch normalisation to address data discrepancy from different datasets, which largely improved the proposed model's generalisability and its potential feasibility in other unseen clinical settings.
After integration with prediagnosis assessment deep learning models,
Other AI models for detection of dementia from retinal imaging are under development,
Our study had some limitations.
First, the overall size of our training dataset was relatively small compared with traditional deep learning studies, which typically require larger datasets including diverse populations.
Moreover, our data might not be heterogeneous enough for developing as a screening tool.
Nevertheless, few datasets are available with both retinal photographs and information on Alzheimer's disease.
Second, pathological studies suggest that clinical Alzheimer's disease diagnostic sensitivity ranges between 70•9% and 87•3%, and specificity between 44•3% and 70•8%.
Because the labelling of our training dataset was based on clinicianderived diagnosis, the development of the deep learning algorithm might include retinal photographs from individuals incorrectly labelled as having Alzheimer's disease.
Nevertheless, we also tested our deep learning algorithm in datasets with PET imaging to mitigate this concern.
Third, other inherent biases including selection bias, unbalanced training data, bias in human labelling, racial and ethnic bias, and unknown confounders (eg, myopia status) cannot be eliminated and evaluated in the present retrospective study design.
Fourth, the performances across testing cohorts were slightly variable due to the differences in data distribution.
For example, the testing 5 cohort, which only included three patients with Alzheimer's disease and had a higher mean age than that of other testing datasets, might overestimate the model's performance.
Future testing of the model generalisability across diverse populations or settings and on retinal photographs acquired with different retinal cameras, and research on interpretability of the deep learning algorithms and their clinical implementation will be required.
Fifth, substantial overlap between Alzheimer's disease and cerebrovascular disease was expected because they commonly manifest as comorbidities due to shared risk factors.
Nevertheless, the recruitment of patients with predominantly Alzheimer's disease-dementia in our study rested on the clinical judgement of our neurologists according to their clinical experience, diagnostic criteria, and the best available evidence.
In conclusion, we developed, validated, and tested a retinal photograph-based deep learning algorithm for detecting Alzheimer's disease-dementia.
Our proof-ofconcept study provides a unique and generalisable model that could be used in community settings to screen for Alzheimer's disease.
Contributors CYC, ARR, VCTM, DM, CL-HC, and TYW conceived and designed the study.
SW developed and validated the deep learning system supervised by P-AH with clinical input from CYC, ARR, VTTC, VCTM, DM, CL-HC, and TYW.
KS organised all the data.
SH, NV, C-YC, CS, YCT, LS, GJMcK, MAW, AW, LWCA, JCY, CCT, JJC, OMD, ZL, and TCYK provided data of different studies.
ARR, SW, and KS did the statistical analysis.
CYC, ARR, SW, and VTTC wrote the initial draft.
VCTM, DM, CL-HC, and TYW contributed equally to the work as senior authors.
All authors subsequently critically edited the report.
All authors read and approved the final report.
The corresponding author and senior authors had full access to all data.
CYC, ARR, and SW accessed and verified the data.
CYC, VCTM, DM, CL-HC, and TYW had final responsibility for the decision to submit for publication.

Declaration of interests
CYC, ARR, SW, CCT, P-AH, and VCTM are filing a patent for the deep learning model in this study.
All authors declare no competing interests.

Data sharing
The deidentified retinal photographs, the study protocol, the statistical analysis plan, and the coding are available upon request.
Requests will be reviewed on a case-by-case basis.
Proposals should be directed to the corresponding author.



Figure :
Figure: Overview of the proposed deep learning models We used domain-specific batch normalisation block for unsupervised domain adaptation.
All the remaining layers are shared through the network.
(A) The bilateral model feeds four retinal photographs, including optic nerve head-centred and macula-centred photographs of right and left eye for the classification of Alzheimer's disease-dementia and no dementia.
(B) The unilateral model fuses the information from both optic nerve headcentred and macula-centred images in each eye for the Alzheimer's disease-dementia prediction.
(C) The hybrid model is based on our proposed bilateral model, by integrating demographic information into the deep models using bilinear transformation.



Testing datasets with amyloid-PET imaging Testing datasets without amyloid-PET imaging
Both optic nerve head-centred and macula-centred retinal photographs in the eye are available.
†Assessed with Clinical Dementia Rating; seven had a rating of 0•5, 198 had a rating of 1, 74 had a rating of 2, eight had a rating of 3, and five had no data available.
‡Assessed with the Montreal Cognitive Assessment; five were in the 17th percentile, five were in the 16th percentile or lower, four were in the 7th percentile or lower, and 20 were in the 2nd percentile or lower.
§Assessed with Mini-Mental State Examination; mean 19 (SD 5), range 1-29, median 20 (IQR 6).
¶Assessed with Clinical Dementia Rating, five had a rating of 0•5, five had a rating of 1, four had a rating of 2, none had a rating of 3, and none had no data available.
||Assessed with the Montreal Cognitive Assessment; two were in the 17th percentile, one was in the 16th percentile or lower, none were in the 7th percentile or lower, and none were in the 2nd percentile or lower.
**The number of patients with Alzheimer's disease-dementia, participants with cognitive impairment but no dementia, and participants without cognitive impairment and did not have dementia.



Table 1 : Characteristics of the primary training and validation, and testing datasets Articles www



November 2022 e812 Results



disease-dementia vs no dementia
Our proposed retinal photograph-based deep learning model provides a simple, low-cost, low labour-dependent approach to identify potential Alzheimer's diseasedementia patients in community settings with reasonable



Table 3 : The participant-level performance of the deep learning-based model stratified by eye disease and diabetic mellitus in the testing datasets with amyloid-PET imaging